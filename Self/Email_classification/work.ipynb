{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModel,TFAutoModelForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(179, 2)\n"
     ]
    }
   ],
   "source": [
    "email_data= pd.read_csv(\"email_classification.csv\")\n",
    "email_data.head()\n",
    "print(email_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using LLM model to get vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(179, 768)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"distilbert-base-uncased\"  # You can replace this with any other model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModel.from_pretrained(model_name)\n",
    "\n",
    "# Example email sentences\n",
    "emails = email_data[\"email\"].tolist()\n",
    "\n",
    "# Tokenize and encode the email sentences\n",
    "max_length = 64  # You can adjust this as needed\n",
    "inputs = tokenizer(emails, padding=True, truncation=True, max_length=max_length, return_tensors=\"tf\")\n",
    "\n",
    "# Pass the inputs through the model to get the embeddings\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Get the embeddings for all tokens\n",
    "embeddings = outputs.last_hidden_state\n",
    "\n",
    "# Compute average pooling to get sentence embeddings\n",
    "mask = tf.cast(inputs['attention_mask'], tf.float32)\n",
    "sum_embeddings = tf.reduce_sum(embeddings * tf.expand_dims(mask, -1), axis=1)\n",
    "avg_embeddings = sum_embeddings / tf.reduce_sum(mask, axis=1, keepdims=True)\n",
    "\n",
    "# Convert TensorFlow tensor to numpy array\n",
    "embeddings = avg_embeddings.numpy()\n",
    "\n",
    "# Now 'embeddings' contains the vector representations of the email sentences\n",
    "print(embeddings.shape)  # Shape: (num_sentences, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reforming the Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\"ham\": 0, \"spam\": 1}\n",
    "\n",
    "# Map the labels using the mapping\n",
    "y = email_data[\"label\"].map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(179, 768)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(179,)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X.shape)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the indices\n",
    "indices = np.arange(len(X))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Define the split ratio\n",
    "train_ratio = 0.8  # 80% train, 20% test\n",
    "\n",
    "# Calculate split indices\n",
    "split_idx = int(len(X) * train_ratio)\n",
    "train_indices = indices[:split_idx]\n",
    "test_indices = indices[split_idx:]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test = tf.gather(X, train_indices), tf.gather(X, test_indices)\n",
    "\n",
    "# Assuming you have labels y, you can split them as well\n",
    "y_train, y_test = y[train_indices], y[test_indices]\n",
    "\n",
    "# Now you have X_train, y_train, X_test, y_test for training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143, 768)\n",
      "(36, 768)\n",
      "(143,)\n",
      "(36,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9722222222222222\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create a KNN classifier\n",
    "k = 5  # Number of neighbors\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "# Train the classifier\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = knn_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9300699300699301\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = knn_classifier.predict(X_train)\n",
    "\n",
    "# Calculate training accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Training Accuracy:\", train_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9722222222222222\n"
     ]
    }
   ],
   "source": [
    "# Initialize the SVM classifier\n",
    "svm_classifier = SVC(kernel='linear')  # You can choose different kernels like 'rbf', 'poly', etc.\n",
    "\n",
    "# Train the SVM classifier\n",
    "svm_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for test set\n",
    "y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = svm_classifier.predict(X_train)\n",
    "\n",
    "# Calculate training accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Training Accuracy:\", train_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the Gaussian Naive Bayes classifier\n",
    "naive_bayes_classifier = GaussianNB()\n",
    "\n",
    "# Train the Naive Bayes classifier\n",
    "naive_bayes_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for test set\n",
    "y_pred = naive_bayes_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.9440559440559441\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = naive_bayes_classifier.predict(X_train)\n",
    "\n",
    "# Calculate training accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Training Accuracy:\", train_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9722222222222222\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Decision Tree classifier\n",
    "decision_tree_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Train the Decision Tree classifier\n",
    "decision_tree_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels for test set\n",
    "y_pred = decision_tree_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Predict the labels for training set\n",
    "y_train_pred = decision_tree_classifier.predict(X_train)\n",
    "\n",
    "# Calculate training accuracy\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "print(\"Training Accuracy:\", train_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
