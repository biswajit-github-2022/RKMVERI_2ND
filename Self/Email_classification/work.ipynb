{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModel,TFAutoModelForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(179, 2)\n"
     ]
    }
   ],
   "source": [
    "email_data= pd.read_csv(\"email_classification.csv\")\n",
    "email_data.head()\n",
    "print(email_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing TFDistilBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFDistilBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embeddings: (179, 768)\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "model_name = \"distilbert-base-uncased\"  # e.g., \"bert-base-uncased\", \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = TFAutoModel.from_pretrained(model_name)\n",
    "\n",
    "# List of sentences/emails\n",
    "emails = email_data[\"email\"].tolist()\n",
    "\n",
    "# Tokenize and encode emails\n",
    "encoded_inputs = tokenizer(emails, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(encoded_inputs)\n",
    "\n",
    "# Extract the output embeddings (CLS token for BERT-based models)\n",
    "# For BERT-based models, we take the output from the last hidden layer and the first token [CLS]\n",
    "# For other models, you might need to adjust this\n",
    "embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "\n",
    "print(\"Shape of embeddings:\", embeddings.shape)\n",
    "# Now, 'embeddings' contains the vectors for your emails\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping = {\"ham\": 0, \"spam\": 1}\n",
    "\n",
    "# Map the labels using the mapping\n",
    "y = email_data[\"label\"].map(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(179, 768)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(179,)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X.shape)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the indices\n",
    "indices = np.arange(len(X))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "# Define the split ratio\n",
    "train_ratio = 0.8  # 80% train, 20% test\n",
    "\n",
    "# Calculate split indices\n",
    "split_idx = int(len(X) * train_ratio)\n",
    "train_indices = indices[:split_idx]\n",
    "test_indices = indices[split_idx:]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test = tf.gather(X, train_indices), tf.gather(X, test_indices)\n",
    "\n",
    "# Assuming you have labels y, you can split them as well\n",
    "y_train, y_test = y[train_indices], y[test_indices]\n",
    "\n",
    "# Now you have X_train, y_train, X_test, y_test for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(143, 768)\n",
      "(36, 768)\n",
      "(143,)\n",
      "(36,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9444444444444444\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Create a KNN classifier\n",
    "k = 1  # Number of neighbors\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "# Train the classifier\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = knn_classifier.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
