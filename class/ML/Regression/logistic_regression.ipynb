{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Cost: 0.6929909998072656\n",
      "Iteration 100, Cost: 0.678986281221206\n",
      "Iteration 200, Cost: 0.6667107957252866\n",
      "Iteration 300, Cost: 0.6552183432302082\n",
      "Iteration 400, Cost: 0.644226372845285\n",
      "Iteration 500, Cost: 0.6336432191448595\n",
      "Iteration 600, Cost: 0.6234323560220922\n",
      "Iteration 700, Cost: 0.6135733910912\n",
      "Iteration 800, Cost: 0.6040508552480575\n",
      "Iteration 900, Cost: 0.5948509867987081\n",
      "Iteration 1000, Cost: 0.5859608141029243\n",
      "Iteration 1100, Cost: 0.5773678969177614\n",
      "Iteration 1200, Cost: 0.5690602545848601\n",
      "Iteration 1300, Cost: 0.5610263456207266\n",
      "Iteration 1400, Cost: 0.5532550601158238\n",
      "Iteration 1500, Cost: 0.5457357141757804\n",
      "Iteration 1600, Cost: 0.5384580435953473\n",
      "Iteration 1700, Cost: 0.5314121962027973\n",
      "Iteration 1800, Cost: 0.5245887229248004\n",
      "Iteration 1900, Cost: 0.5179785677700963\n",
      "Iteration 2000, Cost: 0.5115730569493564\n",
      "Iteration 2100, Cost: 0.5053638873335372\n",
      "Iteration 2200, Cost: 0.49934311443038015\n",
      "Iteration 2300, Cost: 0.49350314003572865\n",
      "Iteration 2400, Cost: 0.4878366996948982\n",
      "Iteration 2500, Cost: 0.4823368500897925\n",
      "Iteration 2600, Cost: 0.4769969564498953\n",
      "Iteration 2700, Cost: 0.4718106800695108\n",
      "Iteration 2800, Cost: 0.46677196599964726\n",
      "Iteration 2900, Cost: 0.4618750309705556\n",
      "Iteration 3000, Cost: 0.45711435159006697\n",
      "Iteration 3100, Cost: 0.45248465285337314\n",
      "Iteration 3200, Cost: 0.44798089699164023\n",
      "Iteration 3300, Cost: 0.44359827267974233\n",
      "Iteration 3400, Cost: 0.4393321846172813\n",
      "Iteration 3500, Cost: 0.43517824349188144\n",
      "Iteration 3600, Cost: 0.4311322563293447\n",
      "Iteration 3700, Cost: 0.427190217231575\n",
      "Iteration 3800, Cost: 0.42334829850012223\n",
      "Iteration 3900, Cost: 0.4196028421406793\n",
      "Iteration 4000, Cost: 0.41595035174182643\n",
      "Iteration 4100, Cost: 0.4123874847196876\n",
      "Iteration 4200, Cost: 0.4089110449188747\n",
      "Iteration 4300, Cost: 0.4055179755591317\n",
      "Iteration 4400, Cost: 0.40220535251635736\n",
      "Iteration 4500, Cost: 0.39897037792619017\n",
      "Iteration 4600, Cost: 0.3958103740980162\n",
      "Iteration 4700, Cost: 0.3927227777270973\n",
      "Iteration 4800, Cost: 0.3897051343924754\n",
      "Iteration 4900, Cost: 0.3867550933283798\n",
      "Iteration 5000, Cost: 0.38387040245701026\n",
      "Iteration 5100, Cost: 0.3810489036707946\n",
      "Iteration 5200, Cost: 0.37828852835249266\n",
      "Iteration 5300, Cost: 0.37558729312183303\n",
      "Iteration 5400, Cost: 0.3729432957977222\n",
      "Iteration 5500, Cost: 0.37035471156543426\n",
      "Iteration 5600, Cost: 0.3678197893385819\n",
      "Iteration 5700, Cost: 0.36533684830605934\n",
      "Iteration 5800, Cost: 0.3629042746545598\n",
      "Iteration 5900, Cost: 0.36052051845766514\n",
      "Predictions: [[1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def initialize_parameters(dim):\n",
    "    # Initialize weights and bias to zeros\n",
    "    w = np.zeros((dim, 1))\n",
    "    b = 0\n",
    "    return w, b\n",
    "\n",
    "def compute_cost(X, y, w, b):\n",
    "    m = len(y)\n",
    "    z = np.dot(X, w) + b\n",
    "    a = sigmoid(z)\n",
    "    cost = -1/m * np.sum(y * np.log(a) + (1 - y) * np.log(1 - a))\n",
    "    return cost\n",
    "\n",
    "def gradient_descent(X, y, w, b, learning_rate, num_iterations):\n",
    "    m = len(y)\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        z = np.dot(X, w) + b\n",
    "        a = sigmoid(z)\n",
    "        \n",
    "        dw = 1/m * np.dot(X.T, (a - y))\n",
    "        db = 1/m * np.sum(a - y)\n",
    "        \n",
    "        w -= learning_rate * dw\n",
    "        b -= learning_rate * db\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            cost = compute_cost(X, y, w, b)\n",
    "            print(f\"Iteration {i}, Cost: {cost}\")\n",
    "\n",
    "    return w, b\n",
    "\n",
    "def predict(X, w, b):\n",
    "    z = np.dot(X, w) + b\n",
    "    a = sigmoid(z)\n",
    "    return (a >= 0.5).astype(int)\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Generate synthetic data\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 2)\n",
    "y = (X[:, 0] + X[:, 1] > 1).astype(int).reshape(-1, 1)\n",
    "\n",
    "# Add a column of ones for bias term\n",
    "X_bias = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "\n",
    "# Initialize parameters\n",
    "w_initial, b_initial = initialize_parameters(X_bias.shape[1])\n",
    "\n",
    "# Set hyperparameters\n",
    "learning_rate = 0.01\n",
    "num_iterations = 6000\n",
    "\n",
    "# Train the model using gradient descent\n",
    "w_trained, b_trained = gradient_descent(X_bias, y, w_initial, b_initial, learning_rate, num_iterations)\n",
    "\n",
    "# Make predictions on new data\n",
    "X_new = np.array([[0.6, 0.7], [0.8, 0.9]])\n",
    "X_new_bias = np.c_[np.ones((X_new.shape[0], 1)), X_new]\n",
    "predictions = predict(X_new_bias, w_trained, b_trained)\n",
    "\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "weights = 0\n",
    "bias =0\n",
    "def initialize_parameters(num_features):\n",
    "        global weights \n",
    "        weights = np.zeros((num_features, 1))\n",
    "        global bias \n",
    "        bias=0\n",
    "\n",
    "def sigmoid(z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "def cost(y, y_pred):\n",
    "      n=len(y)\n",
    "      cost= -(1/n)*np.sum(y*np.log(y_pred)+(1-y)*np.log(1-y_pred))\n",
    "      return cost\n",
    "def gradient_descent(self, X, y, y_pred):\n",
    "        n = len(y)\n",
    "        dw = (1/n) * np.dot(X.T, (y_pred - y))\n",
    "        db = (1/n) * np.sum(y_pred - y)\n",
    "        global weights \n",
    "        weights -= self.learning_rate * dw\n",
    "        global bias \n",
    "        bias -= self.learning_rate * db\n",
    "\n",
    "def fit(X, y):\n",
    "        m, num_features = X.shape\n",
    "        weights,bias=initialize_parameters(num_features)\n",
    "\n",
    "        for iteration in range(num_iterations):\n",
    "            z = np.dot(X, weights) + bias\n",
    "            y_pred = sigmoid(z)\n",
    "\n",
    "            cost = cost(y, y_pred)\n",
    "\n",
    "            if iteration % 100 == 0:\n",
    "                print(f\"Iteration {iteration}, Cost: {cost}\")\n",
    "\n",
    "            gradient_descent(X, y, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
