# You can check your ip address using 
ip addr show


# Start the master node using the following commands in your linux system
cd $SPARK_HOME/conf
cp spark-env.sh.template spark-env.sh
export SPARK_MASTER_HOST=<give your own ip address>
export SPARK_MASTER_PORT=7077
export SPARK_WORKER_CORES=3
export SPARK_WORKER_MEMORY=3g
$SPARK_HOME/sbin/start-master.sh # This command starts your master node

#To confirm if Master Node ran without any error :

######
hduser_@bhaskara17:/opt/spark/conf$ $SPARK_HOME/sbin/start-master.sh


starting org.apache.spark.deploy.master.Master, logging to /opt/spark/logs/spark-hduser_-org.apache.spark.deploy.master.Master-1-bhaskara17.out

######


Run: cat /opt/spark/logs/spark-hduser_-org.apache.spark.deploy.master.Master-1-bhaskara17.out

#replace address after cat with you own log file address which you get after starting master.sh



# Start your worker nodes with the following commands in your linux system
cd $SPARK_HOME/conf
cp spark-env.sh.template spark-env.sh
export SPARK_WORKER_CORES=3
export SPARK_WORKER_MEMORY=3g
$SPARK_HOME/sbin/start-worker.sh spark://<master node ip>:7077


# Use this to check you master node availability and connected workers nodes in your browser
http://localhost:8080/
or
http://<master node ip>:<port>


# Now you need to make a standalone .py file and submit that file to the spark cluster to run that application 
spark-submit <your .py file name>.py  # if you are not running the file from the location of your .py file you can add the path of the file before your .py file like {spark-submit location/of/file/<your .py file name>.py}

#If after submitting the .py file you get any jupyter directory error [if you have jupyter installed in your sysytem] then you can temporariry change that path variable like following
unset PYSPARK_DRIVER_PYTHON #if you wish you can change that for permanently also 


To Know about the flan-t5-small model :  https://huggingface.co/google/flan-t5-small
